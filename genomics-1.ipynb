{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "262adae2",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-23T18:37:39.780536Z",
     "iopub.status.busy": "2025-09-23T18:37:39.779855Z",
     "iopub.status.idle": "2025-09-23T18:37:41.454070Z",
     "shell.execute_reply": "2025-09-23T18:37:41.453405Z"
    },
    "papermill": {
     "duration": 1.68086,
     "end_time": "2025-09-23T18:37:41.455689",
     "exception": false,
     "start_time": "2025-09-23T18:37:39.774829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e9d931",
   "metadata": {
    "papermill": {
     "duration": 0.00213,
     "end_time": "2025-09-23T18:37:41.460806",
     "exception": false,
     "start_time": "2025-09-23T18:37:41.458676",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Setting up enviroment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c37d3503",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-09-23T18:37:41.466675Z",
     "iopub.status.busy": "2025-09-23T18:37:41.466236Z",
     "iopub.status.idle": "2025-09-23T18:39:12.219255Z",
     "shell.execute_reply": "2025-09-23T18:39:12.216671Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 90.761362,
     "end_time": "2025-09-23T18:39:12.224494",
     "exception": false,
     "start_time": "2025-09-23T18:37:41.463132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\r\n",
      "Collecting tensorflow\r\n",
      "  Downloading tensorflow-2.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\r\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\r\n",
      "Collecting pandas\r\n",
      "  Downloading pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\r\n",
      "Collecting numpy\r\n",
      "  Downloading numpy-2.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting biopython\r\n",
      "  Downloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\r\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\r\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\r\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\r\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\r\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\r\n",
      "Collecting protobuf>=5.28.0 (from tensorflow)\r\n",
      "  Downloading protobuf-6.32.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.4)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\r\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\r\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.1)\r\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow)\r\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\r\n",
      "Collecting keras>=3.10.0 (from tensorflow)\r\n",
      "  Downloading keras-3.11.3-py3-none-any.whl.metadata (5.9 kB)\r\n",
      "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\r\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow)\r\n",
      "  Downloading ml_dtypes-0.5.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.9 kB)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\r\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.10.0->tensorflow) (14.0.0)\r\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.10.0->tensorflow) (0.1.0)\r\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.10.0->tensorflow) (0.16.0)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.6.15)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.20.0->tensorflow) (3.8.2)\r\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.20.0->tensorflow) (11.2.1)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.10.0->tensorflow) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\r\n",
      "Downloading tensorflow-2.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.6/620.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading numpy-2.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading keras-3.11.3-py3-none-any.whl (1.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (4.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading protobuf-6.32.1-cp39-abi3-manylinux2014_x86_64.whl (322 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.0/322.0 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: protobuf, numpy, tensorboard, pandas, ml_dtypes, biopython, keras, tensorflow\r\n",
      "  Attempting uninstall: protobuf\r\n",
      "    Found existing installation: protobuf 3.20.3\r\n",
      "    Uninstalling protobuf-3.20.3:\r\n",
      "      Successfully uninstalled protobuf-3.20.3\r\n",
      "  Attempting uninstall: numpy\r\n",
      "    Found existing installation: numpy 1.26.4\r\n",
      "    Uninstalling numpy-1.26.4:\r\n",
      "      Successfully uninstalled numpy-1.26.4\r\n",
      "  Attempting uninstall: tensorboard\r\n",
      "    Found existing installation: tensorboard 2.18.0\r\n",
      "    Uninstalling tensorboard-2.18.0:\r\n",
      "      Successfully uninstalled tensorboard-2.18.0\r\n",
      "  Attempting uninstall: pandas\r\n",
      "    Found existing installation: pandas 2.2.3\r\n",
      "    Uninstalling pandas-2.2.3:\r\n",
      "      Successfully uninstalled pandas-2.2.3\r\n",
      "  Attempting uninstall: ml_dtypes\r\n",
      "    Found existing installation: ml-dtypes 0.4.1\r\n",
      "    Uninstalling ml-dtypes-0.4.1:\r\n",
      "      Successfully uninstalled ml-dtypes-0.4.1\r\n",
      "  Attempting uninstall: keras\r\n",
      "    Found existing installation: keras 3.8.0\r\n",
      "    Uninstalling keras-3.8.0:\r\n",
      "      Successfully uninstalled keras-3.8.0\r\n",
      "  Attempting uninstall: tensorflow\r\n",
      "    Found existing installation: tensorflow 2.18.0\r\n",
      "    Uninstalling tensorflow-2.18.0:\r\n",
      "      Successfully uninstalled tensorflow-2.18.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\r\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.3.3 which is incompatible.\r\n",
      "gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.3 which is incompatible.\r\n",
      "mkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.3 which is incompatible.\r\n",
      "mkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.3 which is incompatible.\r\n",
      "mkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.3 which is incompatible.\r\n",
      "cupy-cuda12x 13.4.1 requires numpy<2.3,>=1.22, but you have numpy 2.3.3 which is incompatible.\r\n",
      "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.2 which is incompatible.\r\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.3 which is incompatible.\r\n",
      "cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.2 which is incompatible.\r\n",
      "google-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 6.32.1 which is incompatible.\r\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\r\n",
      "ydata-profiling 4.16.1 requires numpy<2.2,>=1.16.0, but you have numpy 2.3.3 which is incompatible.\r\n",
      "google-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 6.32.1 which is incompatible.\r\n",
      "google-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\r\n",
      "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\r\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.2 which is incompatible.\r\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.4 which is incompatible.\r\n",
      "google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.1 which is incompatible.\r\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\r\n",
      "pandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.32.1 which is incompatible.\r\n",
      "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\r\n",
      "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.20.0 which is incompatible.\r\n",
      "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.20.0 which is incompatible.\r\n",
      "google-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.20.0 which is incompatible.\r\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\r\n",
      "dataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\r\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\r\n",
      "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed biopython-1.85 keras-3.11.3 ml_dtypes-0.5.3 numpy-2.3.3 pandas-2.3.2 protobuf-6.32.1 tensorboard-2.20.0 tensorflow-2.20.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade tensorflow pandas numpy biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ba05723",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T18:39:12.270696Z",
     "iopub.status.busy": "2025-09-23T18:39:12.270244Z",
     "iopub.status.idle": "2025-09-23T18:39:20.959174Z",
     "shell.execute_reply": "2025-09-23T18:39:20.958188Z"
    },
    "papermill": {
     "duration": 8.716973,
     "end_time": "2025-09-23T18:39:20.963477",
     "exception": false,
     "start_time": "2025-09-23T18:39:12.246504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Setup complete!\n",
      "TensorFlow Version: 2.20.0\n",
      "NumPy Version: 1.26.4\n",
      "Pandas Version: 2.2.3\n",
      "BioPython Version: 1.85\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import Bio\n",
    "\n",
    "print(\"✅ Setup complete!\")\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "print(\"NumPy Version:\", np.__version__)\n",
    "print(\"Pandas Version:\", pd.__version__)\n",
    "print(\"BioPython Version:\", Bio.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf5fc52",
   "metadata": {
    "papermill": {
     "duration": 0.024934,
     "end_time": "2025-09-23T18:39:21.015015",
     "exception": false,
     "start_time": "2025-09-23T18:39:20.990081",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Getting the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb29de9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T18:39:21.066409Z",
     "iopub.status.busy": "2025-09-23T18:39:21.065186Z",
     "iopub.status.idle": "2025-09-23T18:39:52.451379Z",
     "shell.execute_reply": "2025-09-23T18:39:52.450450Z"
    },
    "papermill": {
     "duration": 31.41306,
     "end_time": "2025-09-23T18:39:52.452724",
     "exception": false,
     "start_time": "2025-09-23T18:39:21.039664",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be saved to: /kaggle/working/GenomicStoryteller/lassa_virus_genomes.fasta\n",
      "Searching NCBI for Lassa virus complete genomes...\n",
      "Found 2811 records.\n",
      "Fetching FASTA records...\n",
      "  Fetching records 1 to 200...\n",
      "  Fetching records 201 to 400...\n",
      "  Fetching records 401 to 600...\n",
      "  Fetching records 601 to 800...\n",
      "  Fetching records 801 to 1000...\n",
      "  Fetching records 1001 to 1200...\n",
      "  Fetching records 1201 to 1400...\n",
      "  Fetching records 1401 to 1600...\n",
      "  Fetching records 1601 to 1800...\n",
      "  Fetching records 1801 to 2000...\n",
      "  Fetching records 2001 to 2200...\n",
      "  Fetching records 2201 to 2400...\n",
      "  Fetching records 2401 to 2600...\n",
      "  Fetching records 2601 to 2800...\n",
      "  Fetching records 2801 to 2811...\n",
      "\n",
      "✅ Success! All FASTA files have been downloaded and combined into a single file.\n",
      "You can find your data at: /kaggle/working/GenomicStoryteller/lassa_virus_genomes.fasta\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import necessary libraries\n",
    "from Bio import Entrez\n",
    "from Bio import SeqIO\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# NCBI requires you to identify yourself with an email address.\n",
    "# Replace this with your actual email.\n",
    "Entrez.email = \"ayotunde326@gmail.com\"\n",
    "\n",
    "# The official name of the Lassa virus.\n",
    "search_term = 'txid3052310[Organism]'\n",
    "\n",
    "# Define the path where we will save our data within the Kaggle environment.\n",
    "output_dir = \"/kaggle/working/GenomicStoryteller/\"\n",
    "output_file = os.path.join(output_dir, \"lassa_virus_genomes.fasta\")\n",
    "\n",
    "# --- Ensure the project directory exists ---\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Data will be saved to: {output_file}\")\n",
    "\n",
    "\n",
    "#   the genomic records\n",
    "print(\"Searching NCBI for Lassa virus complete genomes...\")\n",
    "handle = Entrez.esearch(db=\"nuccore\",       # Search the nucleotide database\n",
    "                        term=search_term,\n",
    "                        retmax=\"10000\")     # Get up to 10,000 records\n",
    "record = Entrez.read(handle)\n",
    "handle.close()\n",
    "id_list = record[\"IdList\"]\n",
    "print(f\"Found {len(id_list)} records.\")\n",
    "\n",
    "\n",
    "# Fetch the records and save them \n",
    "print(\"Fetching FASTA records...\")\n",
    "# Fetch in batches\n",
    "batch_size = 200\n",
    "with open(output_file, \"w\") as out_handle:\n",
    "    for start in range(0, len(id_list), batch_size):\n",
    "        end = min(len(id_list), start + batch_size)\n",
    "        print(f\"  Fetching records {start+1} to {end}...\")\n",
    "        \n",
    "        # actual data for the IDs in FASTA format\n",
    "        fetch_handle = Entrez.efetch(db=\"nuccore\",\n",
    "                                     id=id_list[start:end],\n",
    "                                     rettype=\"fasta\",\n",
    "                                     retmode=\"text\")\n",
    "        \n",
    "        # Read the data and write it to file\n",
    "        data = fetch_handle.read()\n",
    "        fetch_handle.close()\n",
    "        out_handle.write(data)\n",
    "\n",
    "print(\"\\n✅ Success! All FASTA files have been downloaded and combined into a single file.\")\n",
    "print(f\"You can find your data at: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bee6e5",
   "metadata": {
    "papermill": {
     "duration": 0.014171,
     "end_time": "2025-09-23T18:39:52.482216",
     "exception": false,
     "start_time": "2025-09-23T18:39:52.468045",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Preprocessing FASTA files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "175dd9b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T18:39:52.512416Z",
     "iopub.status.busy": "2025-09-23T18:39:52.512069Z",
     "iopub.status.idle": "2025-09-23T18:39:52.779850Z",
     "shell.execute_reply": "2025-09-23T18:39:52.778933Z"
    },
    "papermill": {
     "duration": 0.28447,
     "end_time": "2025-09-23T18:39:52.781159",
     "exception": false,
     "start_time": "2025-09-23T18:39:52.496689",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Data Pre-processing ---\n",
      "Successfully parsed 2811 sequences.\n",
      "Total characters in corpus: 9,136,220\n",
      "Vocabulary size: 11\n",
      "Vocabulary: ['A', 'C', 'G', 'K', 'M', 'N', 'R', 'S', 'T', 'W', 'Y']\n",
      "\n",
      "Character-to-Integer Mapping:\n",
      "{'A': 0, 'C': 1, 'G': 2, 'K': 3, 'M': 4, 'N': 5, 'R': 6, 'S': 7, 'T': 8, 'W': 9, 'Y': 10}\n",
      "\n",
      "Clean corpus saved to: /kaggle/working/GenomicStoryteller/lassa_corpus.txt\n",
      "--- Pre-processing Complete! ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Path to the data we downloaded in the previous step\n",
    "input_fasta_file = \"/kaggle/working/GenomicStoryteller/lassa_virus_genomes.fasta\"\n",
    "\n",
    "# Paths for our new, clean output files\n",
    "output_dir = \"/kaggle/working/GenomicStoryteller/\"\n",
    "corpus_file = os.path.join(output_dir, \"lassa_corpus.txt\")\n",
    "\n",
    "print(\"--- Starting Data Pre-processing ---\")\n",
    "\n",
    "# --- Step 2: Parse the FASTA file and Concatenate Sequences ---\n",
    "all_sequences = []\n",
    "# Use Biopython's SeqIO to parse the FASTA file\n",
    "for record in SeqIO.parse(input_fasta_file, \"fasta\"):\n",
    "    # record.seq is a Biopython Seq object, convert it to a string\n",
    "    all_sequences.append(str(record.seq))\n",
    "\n",
    "# Join all individual sequences into one massive string (our corpus)\n",
    "# We use an empty string '' to join them without any separator\n",
    "corpus = \"\".join(all_sequences)\n",
    "\n",
    "print(f\"Successfully parsed {len(all_sequences)} sequences.\")\n",
    "print(f\"Total characters in corpus: {len(corpus):,}\") # The :, adds comma separators for readability\n",
    "\n",
    "# --- Step 3: Create the Vocabulary and Mappings ---\n",
    "# Find all unique characters in our corpus\n",
    "# The set() function automatically finds the unique items\n",
    "chars = sorted(list(set(corpus)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Vocabulary: {chars}\")\n",
    "\n",
    "# Create the character-to-integer mapping (char -> int)\n",
    "char_to_int = {char: i for i, char in enumerate(chars)}\n",
    "\n",
    "# Create the integer-to-character mapping (int -> char)\n",
    "int_to_char = {i: char for i, char in enumerate(chars)}\n",
    "\n",
    "print(\"\\nCharacter-to-Integer Mapping:\")\n",
    "print(char_to_int)\n",
    "\n",
    "# --- Step 4: Save the Clean Corpus ---\n",
    "# Save our massive string to a new, clean text file.\n",
    "# This is our primary deliverable.\n",
    "with open(corpus_file, \"w\") as f:\n",
    "    f.write(corpus)\n",
    "\n",
    "print(f\"\\nClean corpus saved to: {corpus_file}\")\n",
    "print(\"--- Pre-processing Complete! ---\")\n",
    "\n",
    "# You can now use the `corpus`, `char_to_int`, and `int_to_char` variables\n",
    "# in the next steps of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04220332",
   "metadata": {
    "papermill": {
     "duration": 0.014414,
     "end_time": "2025-09-23T18:39:52.810658",
     "exception": false,
     "start_time": "2025-09-23T18:39:52.796244",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Training out LSTM model- Data preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d15ff5fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T18:39:52.841574Z",
     "iopub.status.busy": "2025-09-23T18:39:52.841251Z",
     "iopub.status.idle": "2025-09-23T18:42:35.002231Z",
     "shell.execute_reply": "2025-09-23T18:42:35.001202Z"
    },
    "papermill": {
     "duration": 162.193272,
     "end_time": "2025-09-23T18:42:35.018769",
     "exception": false,
     "start_time": "2025-09-23T18:39:52.825497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 9,136,220 characters\n",
      "Vocabulary size: 11\n",
      "Created 9,136,120 training sequences.\n",
      "Vectorization complete.\n",
      "Shape of X: (9136120, 100)\n",
      "Shape of y: (9136120,)\n",
      "Training data saved to /kaggle/working/GenomicStoryteller/training_data.npz\n",
      "--- Data Preparation Complete! ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json # We'll use this to save our mappings\n",
    "\n",
    "# --- Step 1: Load Pre-processed Data ---\n",
    "# Define the paths to the files we created in the previous step\n",
    "output_dir = \"/kaggle/working/GenomicStoryteller/\"\n",
    "corpus_file = os.path.join(output_dir, \"lassa_corpus.txt\")\n",
    "\n",
    "# Load the clean corpus from the text file\n",
    "with open(corpus_file, 'r') as f:\n",
    "    corpus = f.read()\n",
    "\n",
    "# Recreate the vocabulary and mappings.\n",
    "# This ensures that even if we restart the notebook, we have the correct mappings.\n",
    "chars = sorted(list(set(corpus)))\n",
    "vocab_size = len(chars)\n",
    "char_to_int = {char: i for i, char in enumerate(chars)}\n",
    "int_to_char = {i: char for i, char in enumerate(chars)}\n",
    "\n",
    "# Save the mappings for later use, which is a good practice\n",
    "with open(os.path.join(output_dir, 'mappings.json'), 'w') as f:\n",
    "    json.dump({'char_to_int': char_to_int, 'int_to_char': int_to_char}, f)\n",
    "\n",
    "print(f\"Corpus length: {len(corpus):,} characters\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# --- Step 2: Define Parameters and Create Sequences ---\n",
    "# How many characters of context will the model see at a time?\n",
    "sequence_length = 100\n",
    "\n",
    "# Lists to hold our input sequences (X) and target characters (y)\n",
    "sequences = []\n",
    "targets = []\n",
    "\n",
    "# Create the sequences using the sliding window method\n",
    "for i in range(0, len(corpus) - sequence_length):\n",
    "    input_seq = corpus[i : i + sequence_length]\n",
    "    target_char = corpus[i + sequence_length]\n",
    "    sequences.append(input_seq)\n",
    "    targets.append(target_char)\n",
    "\n",
    "num_sequences = len(sequences)\n",
    "print(f\"Created {num_sequences:,} training sequences.\")\n",
    "\n",
    "# --- Step 3: Vectorize the Data (Translate to Numbers) ---\n",
    "# The model needs numbers, not characters. Let's translate.\n",
    "X = np.zeros((num_sequences, sequence_length), dtype=np.int32)\n",
    "y = np.zeros((num_sequences), dtype=np.int32)\n",
    "\n",
    "for i, seq in enumerate(sequences):\n",
    "    X[i] = [char_to_int[char] for char in seq]\n",
    "    y[i] = char_to_int[targets[i]]\n",
    "\n",
    "print(\"Vectorization complete.\")\n",
    "print(f\"Shape of X: {X.shape}\") # Should be (num_sequences, 100)\n",
    "print(f\"Shape of y: {y.shape}\") # Should be (num_sequences,)\n",
    "\n",
    "# --- (Optional but Recommended) Save the Prepared Data ---\n",
    "# For a large corpus, this step can take time.\n",
    "# Saving the processed data means we don't have to rerun it every time.\n",
    "np.savez_compressed(\n",
    "    os.path.join(output_dir, 'training_data.npz'),\n",
    "    X=X,\n",
    "    y=y,\n",
    "    vocab_size=np.array([vocab_size]) # save vocab_size as a numpy array\n",
    ")\n",
    "\n",
    "print(f\"Training data saved to {os.path.join(output_dir, 'training_data.npz')}\")\n",
    "print(\"--- Data Preparation Complete! ---\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 303.580104,
   "end_time": "2025-09-23T18:42:38.540568",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-23T18:37:34.960464",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
